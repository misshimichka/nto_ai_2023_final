{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install git+https://github.com/openai/CLIP.git transformers gdown wandb katna more_itertools\n!pip install git+https://github.com/cene555/ru-clip-tiny.git \n!gdown -O ru-clip-tiny.pkl https://drive.google.com/uc?id=1-3g3J90pZmHo9jbBzsEmr7ei5zm3VXOL\n\n# эмбеддинги видео\n!gdown 1eCSqwe9ByUkc-hA108OrTxnXSdEOR5TZ","metadata":{"id":"eSIib9XGY6Fs","execution":{"iopub.status.busy":"2023-03-01T20:21:58.668214Z","iopub.execute_input":"2023-03-01T20:21:58.668495Z","iopub.status.idle":"2023-03-01T20:22:11.807473Z","shell.execute_reply.started":"2023-03-01T20:21:58.668466Z","shell.execute_reply":"2023-03-01T20:22:11.806112Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport io\nimport os\nimport sys\nimport cv2\nimport clip\nimport json\nimport wandb\nimport pickle\nimport random\nimport argparse\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport more_itertools\nfrom time import time\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image \nfrom dataclasses import dataclass, field\nfrom typing import Tuple, Optional, Union\n\nimport PIL\nimport PIL.Image\nfrom PIL import Image\n\nfrom tqdm import tqdm, trange\nfrom tqdm.contrib import tzip\nfrom tqdm.notebook import tqdm\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as nnf\nfrom torch.cuda.amp import autocast\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.checkpoint import checkpoint_sequential\n\nfrom torchvision import transforms\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\n\n\nimport transformers\nfrom transformers import BertTokenizer\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, AdamW, get_linear_schedule_with_warmup\nfrom transformers.optimization import Adafactor, AdafactorSchedule\n\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"id":"jAGDCFZ6aOCA","execution":{"iopub.status.busy":"2023-03-01T20:22:16.364212Z","iopub.execute_input":"2023-03-01T20:22:16.365057Z","iopub.status.idle":"2023-03-01T20:22:25.912764Z","shell.execute_reply.started":"2023-03-01T20:22:16.365020Z","shell.execute_reply":"2023-03-01T20:22:25.911404Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# https://github.com/cene555/ru-clip-tiny.git\n\nclass Predictor:\n    def __init__(self):\n        self.tokenizer = Tokenizer()\n        self.transform = get_transform()\n        \n        self.model = RuCLIPtiny()\n        self.model.load_state_dict(torch.load('/kaggle/working/ru-clip-tiny.pkl', map_location=device))\n        self.model = self.model.to(device).eval()\n        for x in self.model.parameters(): x.requires_grad = False\n        torch.cuda.empty_cache()\n\n    def prepare_images_features(self, images, device='cpu'):\n        images_features = []\n        for image in images:\n            image = self.transform(image)\n            with torch.no_grad():\n                image_features = self.model.encode_image(image.unsqueeze(0).to(device)).float().cpu()[0]\n            images_features.append(image_features)\n        images_features = torch.stack(images_features, axis=0)\n        images_features /= images_features.norm(dim=-1, keepdim=True)\n        return images_features.cpu()\n\n    def prepare_text_features(self,texts, max_len=77, device='cpu'):\n        texts_features = []\n        for text in texts:\n            tokens = self.tokenizer.tokenize([text], max_len)\n            with torch.no_grad():\n                text_features = self.model.encode_text(tokens[0].to(device), tokens[1].to(device)).float().cpu()[0]\n            texts_features.append(text_features)\n        texts_features = torch.stack(texts_features, axis=0)\n        texts_features /= texts_features.norm(dim=-1, keepdim=True)\n        return texts_features\n\n    def __call__(self,images, classes, max_len=77, device='cpu'):\n        self.model.eval().to(device)\n        image_features = self.prepare_images_features(images, device)\n        texts_features = self.prepare_text_features(classes, max_len, device)\n        return 1 * image_features @ texts_features.T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nimport numpy as np\nimport clip\nfrom Katna.video import Video\nfrom PIL import Image\nimport cv2\n\ndef read_video(path, frames_num=9, window=30, resize=True):\n    frames = []\n    cap = cv2.VideoCapture(path)\n    \n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    \n    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    N = length // frames_num\n    \n    current_frame = 1\n    for i in range(length):\n        ret, frame = cap.read(current_frame)\n        if ret and i==current_frame and len(frames)<frames_num:\n            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n            if resize:\n                size = 193, 193\n                frame.thumbnail(size, Image.ANTIALIAS)\n            \n            frames.append(frame)\n            current_frame += N\n        \n    cap.release()\n    return frames\n\n\nclass FrameExtractor:\n    def __init__(self, num_key_frames=20, num_frames=10, use_keyframe=False):\n        self.N_keyframe = num_key_frames\n        self.N_frames = num_frames\n        self.use_keyframe = use_keyframe\n        \n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.predictor = Predictor()\n        self.vid_obj = Video()\n\n    def extract_video_keyframes(self,file_path):\n\n        video_duration = self.vid_obj._get_video_duration_with_cv(file_path)\n        video_split_threshold_in_minutes = 20\n        if video_duration > (video_split_threshold_in_minutes * 60):\n            top_frames = self.vid_obj.extract_video_keyframes_big_video(self.N_keyframe, file_path)\n        else:\n            top_frames = self.vid_obj._extract_keyframes_from_video(self.N_keyframe, file_path)\n        return top_frames\n\n    def fix_color(self, frame):\n        if frame.ndim == 3 and frame.shape[2] == 4:  # Convert BGRA frames to RGB\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGRA2RGB)\n        elif frame.ndim == 3 and frame.shape[2] == 1:  # Convert grayscale frames to RGB\n            frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n        else:  # Convert BGR frames to RGB\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        return frame\n    \n    def get_frames(self, vid_path, description, scores=False, use_keyframe=False):\n        if use_keyframe:\n            keyframes = self.extract_video_keyframes(vid_path)\n            keyframes = [self.fix_color(frame) for frame in keyframes]\n        else:\n            keyframes = read_video(path=vid_path, frames_num=self.N_keyframe, resize=False)\n        \n        text_probs = self.predictor(images=keyframes,\n                       classes=[description],\n                       max_len=77, device=device)\n\n        keyframe_scores = text_probs.cpu().numpy().flatten().tolist()\n            \n        data = []\n        for frame, score in zip(keyframes, keyframe_scores):\n            data.append([frame, score])\n\n        data = sorted(data, key=lambda x: x[1], reverse=True)\n        frames = data[:self.N_frames] #(frame, score)\n        if scores:\n            return frames\n        res = []\n        for frame_with_score in frames:\n            frame = frame_with_score[0]\n            size = 193, 193\n            if use_keyframe:\n                frame = Image.fromarray(frame)\n            frame.thumbnail(size, Image.ANTIALIAS)\n            res.append(frame)\n        return res","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_grid(imgs, rows, cols):\n    pils = imgs\n    \n    assert len(imgs) == rows*cols\n\n    w, h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    grid_w, grid_h = grid.size\n    \n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid        \n        \n    cap.release()\n    return frames","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out_path = f\"Features_train_full_ru_2_2.pkl\"\ndf_train = pd.read_csv('/kaggle/input/csv-nto-final/updatedtrain.csv')\n\nall_embeddings = []\nall_captions = []\ni = 0\nframe_extractor = FrameExtractor(num_key_frames=12, num_frames=4)\n\nfor video_name, question, answer in tzip(df_train.video_name, df_train.question, df_train.answer):\n    name = f'{video_path}/{video_name}.mp4'\n    text = f'Q: {question} A: {answer}'\n    \n    if os.path.exists(name):\n        frames = frame_extractor.get_frames(name, question, scores=False, use_keyframe=False)\n        if len(frames) == 4:\n            image = image_grid(frames, 2, 2)\n            with torch.no_grad():\n                image = preprocess(image).unsqueeze(0).to(device)\n                prefix = clip_model.encode_image(image).cpu()\n            all_embeddings.append(prefix)\n            all_captions.append(text)\n    \nwith open(out_path, 'wb') as f:\n    pickle.dump({\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions}, f)\n\nprint('Done')\nprint(\"%0d embeddings saved \" % len(all_embeddings))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClipCocoDataset(Dataset):\n    \n    def __init__(self, data_path: str,  prefix_length= 50, gpt2_type = \"sberbank-ai/rugpt3large_based_on_gpt2\",\n                 normalize_prefix=False):\n        self.tokenizer = AutoTokenizer.from_pretrained(gpt2_type)\n        self.prefix_length = prefix_length\n        self.normalize_prefix = normalize_prefix\n        with open(data_path, 'rb') as f:\n            all_data = pickle.load(f)\n        print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n        sys.stdout.flush()\n        self.prefixes = all_data[\"clip_embedding\"]\n        captions_raw = all_data[\"captions\"]\n        \n        self.captions = captions_raw\n        \n        \n        self.captions_tokens = []\n        self.caption2embedding = []\n        max_seq_len = 0\n        i=0\n        for caption in tqdm(captions_raw):\n                self.captions_tokens.append(torch.tensor(self.tokenizer.encode(caption), dtype=torch.int64))\n                self.caption2embedding.append(self.prefixes[i])\n                i+=1\n                max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])    \n    \n        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n\n    def pad_tokens(self, item: int):\n        tokens = self.captions_tokens[item]\n        padding = self.max_seq_len - tokens.shape[0]\n        if padding > 0:\n            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n            self.captions_tokens[item] = tokens\n        elif padding < 0:\n            tokens = tokens[:self.max_seq_len]\n            self.captions_tokens[item] = tokens\n        mask = tokens.ge(0)\n        tokens[~mask] = 0\n        mask = mask.float()\n        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n        return tokens, mask\n    \n    def __len__(self) -> int:\n        return len(self.captions_tokens)\n\n    def __getitem__(self, item):\n        tokens, mask = self.pad_tokens(item)\n        prefix = self.prefixes[item]\n        if self.normalize_prefix:\n            prefix = prefix.float()\n            prefix = prefix / prefix.norm(2, -1)\n        return tokens, mask, prefix","metadata":{"id":"5uF07hEqcltu","execution":{"iopub.status.busy":"2023-03-01T20:22:26.032874Z","iopub.execute_input":"2023-03-01T20:22:26.035381Z","iopub.status.idle":"2023-03-01T20:22:26.555410Z","shell.execute_reply.started":"2023-03-01T20:22:26.035340Z","shell.execute_reply":"2023-03-01T20:22:26.554247Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n        super(MLP, self).__init__()\n        layers = []\n        for i in range(len(sizes) - 1):\n            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n            if i < len(sizes) - 2:\n                layers.append(act())\n        self.model = nn.Sequential(*layers)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x)\n\n    \ndef freeze(\n    model,\n    freeze_emb=False,\n    freeze_ln=False,\n    freeze_attn=True,\n    freeze_ff=True,\n    freeze_other=True,\n):\n    \n    for name, p in model.named_parameters(): \n        name = name.lower()\n        if 'ln' in name or 'norm' in name:\n            p.requires_grad = not freeze_ln\n        elif 'embeddings' in name:\n            p.requires_grad = not freeze_emb\n        elif 'mlp' in name:\n            p.requires_grad = not freeze_ff\n        elif 'attn' in name:\n            p.requires_grad = not freeze_attn\n        else:\n            p.requires_grad = not freeze_other\n           \n    return model\n\nclass ClipCaptionModel(nn.Module):\n    def __init__(self, backbone, prefix_length: int, prefix_size: int = 768):\n        super(ClipCaptionModel, self).__init__()\n        self.prefix_length = prefix_length\n        self.gpt = AutoModelForCausalLM.from_pretrained(backbone)\n        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n        self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n                                  self.gpt_embedding_size * prefix_length))\n\n    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n    \n    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n                labels: Optional[torch.Tensor] = None):\n\n        embedding_text = self.gpt.transformer.wte(tokens)\n        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n\n        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n        if labels is not None:\n            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n            labels = torch.cat((dummy_token, tokens), dim=1)\n        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n        return out\n\n  \nclass ClipCaptionPrefix(ClipCaptionModel):\n    def parameters(self, recurse: bool = True):\n        return self.clip_project.parameters()\n    \n    def train(self, mode: bool = True):\n        super(ClipCaptionPrefix, self).train(mode)\n        self.gpt.eval()\n        return self","metadata":{"execution":{"iopub.status.busy":"2023-03-01T20:22:26.557055Z","iopub.execute_input":"2023-03-01T20:22:26.557416Z","iopub.status.idle":"2023-03-01T20:22:26.576887Z","shell.execute_reply.started":"2023-03-01T20:22:26.557369Z","shell.execute_reply":"2023-03-01T20:22:26.575772Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def filter_ngrams(output_text):\n    a_pos = output_text.find(' A:')\n    sec_a_pos = output_text.find(' A:', a_pos + 1)\n    return output_text[:sec_a_pos]\n\n\ndef generate2(\n        model,\n        tokenizer,\n        prompt='',\n        embed=None,\n        entry_count=1,\n        entry_length=50,  # maximum number of words\n        top_p=0.98,\n        temperature=1.,\n        stop_token='<|endoftext|>',\n):\n    tokens = None\n    model.eval()\n    generated_num = 0\n    generated_list = []\n    stop_token_index = tokenizer.encode(stop_token)[0]\n    filter_value = -float(\"Inf\")\n    device = next(model.parameters()).device\n\n    with torch.no_grad():\n\n        for entry_idx in range(entry_count):\n            if not tokens:\n                tokens = torch.tensor(tokenizer.encode(prompt))\n                tokens = tokens.unsqueeze(0).to(device)\n\n            emb_tokens = model.gpt.transformer.wte(tokens)\n\n            if embed[entry_idx] is not None:\n                emb = embed[entry_idx].unsqueeze(0).to(device)\n                generated = torch.cat((emb, emb_tokens), dim=1)\n            else:\n                generated = emb_tokens\n\n            for i in range(entry_length):\n                outputs = model.gpt(inputs_embeds=generated)\n                logits = outputs.logits\n                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n                sorted_indices_to_remove = cumulative_probs > top_p\n                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n                                                    ..., :-1\n                                                    ].clone()\n                sorted_indices_to_remove[..., 0] = 0\n\n                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n                logits[:, indices_to_remove] = filter_value\n                #\n                top_k = 2000\n                top_p = 0.98\n                next_token = torch.argmax(logits, -1).unsqueeze(0)\n                next_token_embed = model.gpt.transformer.wte(next_token)\n\n                if tokens is None:\n                    tokens = next_token\n                else:\n                    tokens = torch.cat((tokens, next_token), dim=1)\n                generated = torch.cat((generated, next_token_embed), dim=1)\n\n                if stop_token_index == next_token.item():\n                    break\n\n            output_list = list(tokens.squeeze().cpu().numpy())\n\n            output_text = tokenizer.decode(output_list)\n            output_text = filter_ngrams(output_text)\n            generated_list.append(output_text)\n\n    return generated_list","metadata":{"execution":{"iopub.status.busy":"2023-03-01T20:22:26.578859Z","iopub.execute_input":"2023-03-01T20:22:26.579504Z","iopub.status.idle":"2023-03-01T20:22:26.594804Z","shell.execute_reply.started":"2023-03-01T20:22:26.579465Z","shell.execute_reply":"2023-03-01T20:22:26.593789Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import nltk\n\ndef get_score(gt, pred):\n    score = nltk.translate.bleu_score.sentence_bleu([gt.lower().replace('<|endoftext|>','').replace(\"!\", \"\").split()], pred.lower().replace('<|endoftext|','').split(), weights = (0.5, 0.5))\n    return score","metadata":{"execution":{"iopub.status.busy":"2023-03-01T20:22:26.598281Z","iopub.execute_input":"2023-03-01T20:22:26.598550Z","iopub.status.idle":"2023-03-01T20:22:27.605291Z","shell.execute_reply.started":"2023-03-01T20:22:26.598524Z","shell.execute_reply":"2023-03-01T20:22:27.603798Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def train(dataset, model: ClipCaptionModel, args,\n          warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n\n    device = torch.device('cuda')\n    batch_size = args.bs\n    epochs = args.epochs\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    model = model.to(device)\n    \n    model = freeze(model)\n    model.train()\n    optimizer = AdamW(model.parameters(), lr=args.lr,betas=(0.9, 0.995))\n\n    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n    )\n    current_loss = 1000000\n\n    for epoch in range(epochs):\n        print(f\">>> Training epoch {epoch}\")\n        sys.stdout.flush()\n        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n        step=0\n        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n            model.zero_grad()\n            step+=1\n            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n            \n            outputs = model(tokens, prefix, mask)\n            logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n\n            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n\n            segments = 2\n            \n            loss.backward()\n            \n            optimizer.step()\n            scheduler.step()\n            progress.set_postfix({\"loss\": loss.item()})\n            \n            clipping_value = 0.5 \n            optimizer.step()\n            scheduler.step()\n            \n            wandb.log({\"loss\":  loss.item()})\n            \n            progress.update()\n            \n            del tokens\n            del mask\n            del prefix\n            torch.clear_autocast_cache()\n            torch.cuda.empty_cache()\n            \n            if (idx + 1) % 7000 == 0:\n                torch.save(\n                    model.state_dict(),\n                    os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n                )\n        \n        step = 0\n        val_loss = []\n        step = 0\n        scores = []\n        val_loss = []\n        for idx, (tokens, mask, prefix) in enumerate(val_loader):\n            with torch.no_grad():\n                step += 1\n\n                tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n\n                outputs = model(tokens, prefix, mask)\n                logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n                loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n                val_loss.append(loss.item())\n\n                progress.set_postfix({\"val_loss\": loss.item()})\n                wandb.log({\"val_loss\": loss.item()})\n\n                progress.update()\n\n                del tokens\n                del mask\n                del prefix\n                \n\n        torch.clear_autocast_cache()\n        torch.cuda.empty_cache()\n\n        progress.close()\n        \n        if epoch in [4, 5, 6]:\n            torch.save(\n                model.state_dict(),\n                os.path.join(output_dir, f\"{output_prefix}_{epoch+1}.pt\"))   \n\n        val_loss = sum(val_loss) / len(val_loss)\n        if val_loss < current_loss:\n            print(f'[+] Val loss has increased from {current_loss:.4f} to {val_loss:.4f}')\n        else:            \n            print(f'[-] Val loss has not improved. {val_loss:.4f}')\n        progress.close()\n    return model ","metadata":{"id":"K3N3MQ7afj8m","outputId":"986bd87f-084a-470f-e25f-953e760b389d","execution":{"iopub.status.busy":"2023-03-01T20:22:27.612408Z","iopub.execute_input":"2023-03-01T20:22:27.612813Z","iopub.status.idle":"2023-03-01T20:22:27.644115Z","shell.execute_reply.started":"2023-03-01T20:22:27.612774Z","shell.execute_reply":"2023-03-01T20:22:27.642601Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class Args():\n    def __init__(self):\n        self.backbone = 'sberbank-ai/rugpt3small_based_on_gpt2'\n        self.data = '/kaggle/working/Features_train_full_ru_2_2.pkl'\n        self.out_dir = 'checkpoints_larger'\n        self.wandb_key = 'YOUR WANDB KEY HERE'\n        self.project_name = \"final-nto\"\n        self.prefix = 'prefix_2_2_small'\n        self.epochs = 7\n        self.save_every = 1\n        self.prefix_length = 50\n        self.batch_size = 16\n        self.bs = 16\n        self.debug = True\n        self.only_prefix = False\n        self.lr = 5e-5\n        self.train_size = 0.9\n        \nargs = Args()","metadata":{"execution":{"iopub.status.busy":"2023-03-01T20:22:27.649296Z","iopub.execute_input":"2023-03-01T20:22:27.651600Z","iopub.status.idle":"2023-03-01T20:22:27.660434Z","shell.execute_reply.started":"2023-03-01T20:22:27.651558Z","shell.execute_reply":"2023-03-01T20:22:27.659449Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import wandb\n\nif args.debug:\n    wandb.login(key=args.wandb_key)\n    wandb.init(project=args.project_name, config=args)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.config = {\n    \"learning_rate\": args.lr,\n    \"epochs\": args.epochs,\n    \"batch_size\": args.bs\n}\nprefix_length = args.prefix_length\ndataset = ClipCocoDataset(args.data, prefix_length)\nmodel = ClipCaptionModel(backbone = args.backbone, prefix_length = 50) \nmodel = model.to(device)\ntokenizer = AutoTokenizer.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2')\n\nprint(\"Train both prefix and GPT\")\n","metadata":{"execution":{"iopub.status.busy":"2023-03-01T20:23:02.040286Z","iopub.execute_input":"2023-03-01T20:23:02.040641Z","iopub.status.idle":"2023-03-01T20:24:01.829163Z","shell.execute_reply.started":"2023-03-01T20:23:02.040598Z","shell.execute_reply":"2023-03-01T20:24:01.827856Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56ad35df237847b7b645babf4b3985bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.71M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dd7721bb15845619d86f0f7d2ad5ea8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2127e4bd3e744fa399c54de477dc2a09"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"Data size is 25228\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 25228/25228 [00:03<00:00, 7482.41it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb77d40ddea94f86a2d4983bae5d0c91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/551M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12902ef08d0140e5bfbb64f243dd598c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.71M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f3f3038a8ba4906bb8b1818e6e53653"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be0e8b98f99249fcbf15a0575703f5b5"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"Train both prefix and GPT\n","output_type":"stream"}]},{"cell_type":"code","source":"train_size = round(args.train_size * len(dataset))\nval_size = len(dataset) - train_size\nprint(f\"Train size: {train_size}, Val size: {val_size}\")\n\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\nval_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-01T20:24:01.831325Z","iopub.execute_input":"2023-03-01T20:24:01.831704Z","iopub.status.idle":"2023-03-01T20:24:01.854298Z","shell.execute_reply.started":"2023-03-01T20:24:01.831654Z","shell.execute_reply":"2023-03-01T20:24:01.853090Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Train size: 22705, Val size: 2523\n","output_type":"stream"}]},{"cell_type":"code","source":"sys.stdout.flush()\ntrain(dataset, model, args, output_dir=args.out_dir, output_prefix=args.prefix)","metadata":{"execution":{"iopub.status.busy":"2023-03-01T20:24:01.855871Z","iopub.execute_input":"2023-03-01T20:24:01.856499Z","iopub.status.idle":"2023-03-01T21:09:26.818877Z","shell.execute_reply.started":"2023-03-01T20:24:01.856459Z","shell.execute_reply":"2023-03-01T21:09:26.817936Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":">>> Training epoch 0\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\nprefix_2_2_small: 1733it [06:25,  4.50it/s, val_loss=2.7]                          ","output_type":"stream"},{"name":"stdout","text":"[+] Val loss has increased from 1000000.0000 to 2.8236\n>>> Training epoch 1\n","output_type":"stream"},{"name":"stderr","text":"\nprefix_2_2_small: 1733it [06:23,  4.52it/s, val_loss=2.52]                          ","output_type":"stream"},{"name":"stdout","text":"[+] Val loss has increased from 1000000.0000 to 2.3086\n>>> Training epoch 2\n","output_type":"stream"},{"name":"stderr","text":"\nprefix_2_2_small: 1733it [06:22,  4.53it/s, val_loss=2.27]                          ","output_type":"stream"},{"name":"stdout","text":"[+] Val loss has increased from 1000000.0000 to 2.2367\n>>> Training epoch 3\n","output_type":"stream"},{"name":"stderr","text":"\nprefix_2_2_small: 1733it [06:23,  4.52it/s, val_loss=2.12]                          ","output_type":"stream"},{"name":"stdout","text":"[+] Val loss has increased from 1000000.0000 to 2.2287\n>>> Training epoch 4\n","output_type":"stream"},{"name":"stderr","text":"\nprefix_2_2_small: 1733it [06:22,  4.53it/s, val_loss=2.05]                          \n","output_type":"stream"},{"name":"stdout","text":"[+] Val loss has increased from 1000000.0000 to 2.2299\n>>> Training epoch 5\n","output_type":"stream"},{"name":"stderr","text":"prefix_2_2_small: 1733it [06:22,  4.53it/s, val_loss=2.08]                          \n","output_type":"stream"},{"name":"stdout","text":"[+] Val loss has increased from 1000000.0000 to 2.2318\n>>> Training epoch 6\n","output_type":"stream"},{"name":"stderr","text":"prefix_2_2_small: 1733it [06:23,  4.52it/s, val_loss=1.94]                          \n","output_type":"stream"},{"name":"stdout","text":"[+] Val loss has increased from 1000000.0000 to 2.2282\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"ClipCaptionModel(\n  (gpt): GPT2LMHeadModel(\n    (transformer): GPT2Model(\n      (wte): Embedding(50264, 768)\n      (wpe): Embedding(2048, 768)\n      (drop): Dropout(p=0.1, inplace=False)\n      (h): ModuleList(\n        (0): GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (lm_head): Linear(in_features=768, out_features=50264, bias=False)\n  )\n  (clip_project): MLP(\n    (model): Sequential(\n      (0): Linear(in_features=768, out_features=19200, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=19200, out_features=38400, bias=True)\n    )\n  )\n)"},"metadata":{}}]}]}